{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Lambda\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.merge import _Merge\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D, Convolution2D, Conv2DTranspose\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "\n",
    "import keras.backend as K\n",
    "from optimizer import optimAdam\n",
    "from functools import partial\n",
    "# import inception_score\n",
    "\n",
    "import _pickle as cPickle\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.backend import tf as ktf\n",
    "from PIL import Image\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "GRADIENT_PENALTY_WEIGHT = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomWeightedAverage(_Merge):\n",
    "    \"\"\"Takes a randomly-weighted average of two tensors. In geometric terms, this outputs a random point on the line\n",
    "    between each pair of input points.\n",
    "    Inheriting from _Merge is a little messy but it was the quickest solution I could think of.\n",
    "    Improvements appreciated.\"\"\"\n",
    "\n",
    "    def _merge_function(self, inputs):\n",
    "        weights = K.random_uniform((BATCH_SIZE, 1, 1, 1))\n",
    "        return (weights * inputs[0]) + ((1 - weights) * inputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN():\n",
    "    \n",
    "    def __init__(self,gan_type):\n",
    "\n",
    "        assert gan_type in ['gan','wgan','improved_wgan','optim']\n",
    "        print(\"GAN Type: \" + gan_type)\n",
    "        self.type = gan_type\n",
    "        self.noise_shape = (100,)\n",
    "        self.img_shape = (28, 28, 1)\n",
    "        self.clip_value = 0.0001 # threshold for weight cliping (-c,c)\n",
    "        self.d_losses = []\n",
    "        real_img = Input(shape=self.img_shape)\n",
    "        \n",
    "        # set gan type specific parameters\n",
    "        optimizer = self.select_optimizer()\n",
    "        loss = self.select_loss()\n",
    "        self.n_critic = self.select_n_critic()\n",
    "        \n",
    "        # Now we initialize the generator and discriminator.\n",
    "        generator = self.make_generator()\n",
    "        discriminator = self.make_discriminator()\n",
    "\n",
    "        # The generator_model is used when we want to train the generator layers.\n",
    "        # As such, we ensure that the discriminator layers are not trainable.\n",
    "        for layer in discriminator.layers:\n",
    "            layer.trainable = False\n",
    "        discriminator.trainable = False\n",
    "        generator_input = Input(shape=self.noise_shape)\n",
    "        generator_layers = generator(generator_input)\n",
    "        discriminator_layers_for_generator = discriminator(generator_layers)\n",
    "        generator_model = Model(inputs=[generator_input], outputs=[discriminator_layers_for_generator])\n",
    "        # We use the Adam paramaters from Gulrajani et al.\n",
    "        generator_model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "        # Now that the generator_model is compiled, we can make the discriminator layers trainable.\n",
    "        for layer in discriminator.layers:\n",
    "            layer.trainable = True\n",
    "        for layer in generator.layers:\n",
    "            layer.trainable = False\n",
    "        discriminator.trainable = True\n",
    "        generator.trainable = False\n",
    "\n",
    "        # The discriminator_model is more complex. It takes both real image samples and random noise seeds as input.\n",
    "        # The noise seed is run through the generator model to get generated images. Both real and generated images\n",
    "        # are then run through the discriminator.\n",
    "        real_samples = Input(shape=self.img_shape)\n",
    "        generator_input_for_discriminator = Input(shape=self.noise_shape)\n",
    "        generated_samples_for_discriminator = generator(generator_input_for_discriminator)\n",
    "        discriminator_output_from_generator = discriminator(generated_samples_for_discriminator)\n",
    "        discriminator_output_from_real_samples = discriminator(real_samples)\n",
    "        \n",
    "        if self.type in ['gan','wgan']:\n",
    "            discriminator_model = Model(inputs=[real_samples, generator_input_for_discriminator],\n",
    "                                        outputs=[discriminator_output_from_real_samples,\n",
    "                                                 discriminator_output_from_generator])\n",
    "\n",
    "            discriminator_model.compile(optimizer=optimizer,\n",
    "                                        loss=[loss,\n",
    "                                              loss])\n",
    "\n",
    "        elif self.type in ['improved_wgan','optim']:\n",
    "            print(\"Gradient Penalty Applied\")\n",
    "            \n",
    "            # We also need to generate weighted-averages of real and generated samples, to use for the gradient norm penalty.\n",
    "            averaged_samples = RandomWeightedAverage()([real_samples, generated_samples_for_discriminator])\n",
    "            # We then run these samples through the discriminator as well. Note that we never really use the discriminator\n",
    "            # output for these samples - we're only running them to get the gradient norm for the gradient penalty loss.\n",
    "            averaged_samples_out = discriminator(averaged_samples)\n",
    "\n",
    "            # The gradient penalty loss function requires the input averaged samples to get gradients. However,\n",
    "            # Keras loss functions can only have two arguments, y_true and y_pred. We get around this by making a partial()\n",
    "            # of the function with the averaged samples here.\n",
    "            partial_gp_loss = partial(self.gradient_penalty_loss,\n",
    "                                      averaged_samples=averaged_samples,\n",
    "                                      gradient_penalty_weight=GRADIENT_PENALTY_WEIGHT)\n",
    "            partial_gp_loss.__name__ = 'gradient_penalty'  # Functions need names or Keras will throw an error\n",
    "\n",
    "            discriminator_model = Model(inputs=[real_samples, generator_input_for_discriminator],\n",
    "                    outputs=[discriminator_output_from_real_samples,\n",
    "                             discriminator_output_from_generator,\n",
    "                             averaged_samples_out])\n",
    "\n",
    "            discriminator_model.compile(optimizer=optimizer,\n",
    "                                        loss=[loss,\n",
    "                                              loss,\n",
    "                                              partial_gp_loss])\n",
    "            \n",
    "        self.generator_model, self.discriminator_model = generator_model, discriminator_model\n",
    "        self.generator, self.discriminator = generator, discriminator\n",
    "        \n",
    "    def select_optimizer(self):\n",
    "        if self.type == 'gan':\n",
    "            print(\"Optimizer: Adam\")\n",
    "            return Adam(lr=0.0002, beta_1=0.5)\n",
    "        elif self.type == 'wgan':\n",
    "            print(\"Optimizer: RMSProp\")\n",
    "            return RMSprop(lr=0.00005)\n",
    "        elif self.type == 'improved_wgan':\n",
    "            print(\"Optimizer: Adam\")\n",
    "            return Adam(lr=0.0001, beta_1=0.5, beta_2=0.9)\n",
    "        elif self.type == 'optim':\n",
    "            print(\"Optimizer: OptimAdam\")\n",
    "            return optimAdam(lr=0.0001, beta_1=0.5, beta_2=0.9)\n",
    "\n",
    "    def select_loss(self):\n",
    "        if self.type == 'gan':\n",
    "            print(\"Loss: Binary Cross Entropy\")\n",
    "            return 'binary_crossentropy'\n",
    "        elif self.type in ['wgan','improved_wgan','optim']:\n",
    "            print(\"Loss: Wasserstein\")\n",
    "            return self.wasserstein_loss\n",
    "        \n",
    "    def select_n_critic(self):\n",
    "        if self.type == 'gan':\n",
    "            print(\"Critics Ratio: 1\")\n",
    "            return 1\n",
    "        elif self.type in ['wgan','improved_wgan','optim']:\n",
    "            print(\"Critics Ratio: 5\")\n",
    "            return 5\n",
    "        \n",
    "    # for WGAN, Improved WGAN, Optim\n",
    "    def wasserstein_loss(self, y_true, y_pred):\n",
    "        return K.mean(y_true * y_pred)\n",
    "    \n",
    "    # for Improved WGAN, Optim\n",
    "    def gradient_penalty_loss(self, y_true, y_pred, averaged_samples, gradient_penalty_weight):\n",
    "        gradients = K.gradients(K.sum(y_pred), averaged_samples)\n",
    "        gradient_l2_norm = K.sqrt(K.sum(K.square(gradients)))\n",
    "        gradient_penalty = gradient_penalty_weight * K.square(1 - gradient_l2_norm)\n",
    "        return gradient_penalty\n",
    "    \n",
    "    def make_generator(self):\n",
    "        \"\"\"Creates a generator model that takes a 100-dimensional noise vector as a \"seed\", and outputs images\n",
    "        of size 28x28x1.\"\"\"\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Reshape((1, 1, self.noise_shape[0]), input_shape=self.noise_shape))\n",
    "        \n",
    "        # 1st hidden layer\n",
    "        model.add(Conv2DTranspose(1024, (4,4), strides=(1, 1), padding='valid'))\n",
    "        model.add(BatchNormalization(momentum=0.99))\n",
    "        model.add(LeakyReLU(alpha=.2))\n",
    "\n",
    "        # 2nd hidden layer\n",
    "        model.add(Conv2DTranspose(512, (4,4), strides=(2, 2), padding='same'))\n",
    "        model.add(BatchNormalization(momentum=0.99))\n",
    "        model.add(LeakyReLU(alpha=.2))\n",
    "\n",
    "        # 3rd hidden layer\n",
    "        model.add(Conv2DTranspose(256, (4,4), strides=(2, 2), padding='same'))\n",
    "        model.add(BatchNormalization(momentum=0.99))\n",
    "        model.add(LeakyReLU(alpha=.2))\n",
    "\n",
    "        # 4th hidden layer\n",
    "        model.add(Conv2DTranspose(128, (4,4), strides=(2, 2), padding='same'))\n",
    "        model.add(BatchNormalization(momentum=0.99))\n",
    "        model.add(LeakyReLU(alpha=.2))\n",
    "\n",
    "        # output layer\n",
    "        model.add(Conv2DTranspose(1, (4,4), strides=(2, 2), padding='same'))\n",
    "        model.add(Activation('tanh'))\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def make_discriminator(self):\n",
    "        \"\"\"Creates a discriminator model that takes an image as input and outputs a single value, representing whether\n",
    "        the input is real or generated. Unlike normal GANs, the output is not sigmoid and does not represent a probability!\n",
    "        Instead, the output should be as large and negative as possible for generated inputs and as large and positive\n",
    "        as possible for real inputs.\n",
    "        Note that the improved WGAN paper suggests that BatchNormalization should not be used in the discriminator.\"\"\"\n",
    "               \n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Lambda(lambda image: ktf.image.resize_images(image, (64, 64)), input_shape=self.img_shape))\n",
    "        \n",
    "        # 1st hidden layer\n",
    "        model.add(Conv2D(128, (4,4), strides=(2, 2), padding='same'))\n",
    "        model.add(LeakyReLU(alpha=.2))\n",
    "\n",
    "        # 2nd hidden layer\n",
    "        model.add(Conv2D(256, (4,4), strides=(2, 2), padding='same'))\n",
    "        model.add(LeakyReLU(alpha=.2))\n",
    "\n",
    "        # 3rd hidden layer\n",
    "        model.add(Conv2D(512, (4,4), strides=(2, 2), padding='same'))\n",
    "        model.add(LeakyReLU(alpha=.2))\n",
    "\n",
    "        # 4th hidden layer\n",
    "        model.add(Conv2D(1024, (4,4), strides=(2, 2), padding='same'))\n",
    "        model.add(LeakyReLU(alpha=.2))\n",
    "\n",
    "        # output layer\n",
    "        model.add(Conv2D(1, (4,4), strides=(1, 1), padding='valid'))\n",
    "        model.add(Activation('sigmoid'))\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def generate_images(self, output_dir, epoch):\n",
    "        \"\"\"Feeds random seeds into the generator and tiles and saves the output to a PNG file.\"\"\"\n",
    "        def tile_images(image_stack):\n",
    "            \"\"\"Given a stacked tensor of images, reshapes them into a horizontal tiling for display.\"\"\"\n",
    "            assert len(image_stack.shape) == 3\n",
    "            image_list = [image_stack[i, :, :] for i in range(image_stack.shape[0])]\n",
    "            tiled_images = np.concatenate(image_list, axis=1)\n",
    "            return tiled_images\n",
    "        \n",
    "        test_image_stack = self.generator.predict(np.random.rand(100, 100))\n",
    "        test_image_stack = (test_image_stack * 127.5) + 127.5\n",
    "        test_image_stack = np.squeeze(np.round(test_image_stack).astype(np.uint8))\n",
    "        tiled_output = tile_images(test_image_stack)\n",
    "        tiled_output = Image.fromarray(tiled_output, mode='L')  # L specifies greyscale\n",
    "        outfile = os.path.join(output_dir, 'epoch_{}.png'.format(epoch))\n",
    "        tiled_output.save(outfile)\n",
    "        outfile = os.path.join(output_dir, 'epoch_{}.pkl'.format(epoch))\n",
    "        with open(outfile, 'wb') as f:\n",
    "            cPickle.dump(test_image_stack, f)\n",
    "    \n",
    "    def train(self, epochs, batch_size=128, save_interval=50):\n",
    "        # First we load the image data, reshape it and normalize it to the range [-1, 1]\n",
    "        (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "        X_train = np.concatenate((X_train, X_test), axis=0)\n",
    "#         X_train = tf.image.resize_images(X_train, [64, 64]).eval()\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1], X_train.shape[2]))\n",
    "        else:\n",
    "            X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2], 1))\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5   \n",
    "        \n",
    "        # We make three label vectors for training. positive_y is the label vector for real samples, with value 1.\n",
    "        # negative_y is the label vector for generated samples, with value -1. The dummy_y vector is passed to the\n",
    "        # gradient_penalty loss function and is not used.\n",
    "        positive_y = np.ones((batch_size, 1, 1, 1), dtype=np.float32)\n",
    "        negative_y = -positive_y\n",
    "        if self.type in ['improved_wgan','optim']:\n",
    "            dummy_y = np.zeros((batch_size, 1), dtype=np.float32)\n",
    "            \n",
    "        self.discriminator_losses = []\n",
    "        self.generator_losses = []\n",
    "        output_dir = '../dclog2_'+self.type\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            np.random.shuffle(X_train)\n",
    "            print(\"Epoch: \", epoch)\n",
    "            print(\"Number of batches: \", int(X_train.shape[0] // batch_size))\n",
    "            discriminator_loss = []\n",
    "            generator_loss = []\n",
    "            minibatches_size = batch_size * self.n_critic\n",
    "            \n",
    "            for i in range(int(X_train.shape[0] // (batch_size * self.n_critic))):\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "                discriminator_minibatches = X_train[i * minibatches_size:(i + 1) * minibatches_size]\n",
    "                for j in range(self.n_critic):\n",
    "                    \n",
    "                    image_batch = discriminator_minibatches[j * batch_size:(j + 1) * batch_size]\n",
    "                    noise = np.random.rand(batch_size, 100).astype(np.float32)\n",
    "                    if self.type in ['gan','wgan']:\n",
    "                        print(image_batch.shape,noise.shape,positive_y.shape,negative_y.shape)\n",
    "                        discriminator_loss.append(self.discriminator_model.train_on_batch([image_batch, noise],\n",
    "                                                  [positive_y, negative_y]))\n",
    "                    elif self.type in ['improved_wgan','optim']:\n",
    "                        discriminator_loss.append(self.discriminator_model.train_on_batch([image_batch, noise],\n",
    "                                                  [positive_y, negative_y, dummy_y]))\n",
    "\n",
    "                    if self.type == 'wgan':\n",
    "                        # Clip discriminator weights\n",
    "                        for l in self.discriminator_model.layers:\n",
    "                            weights = l.get_weights()\n",
    "                            weights = [np.clip(w, -self.clip_value, self.clip_value) for w in weights]\n",
    "                            l.set_weights(weights)\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Generator\n",
    "                # ---------------------\n",
    "                noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "                generator_loss.append(self.generator_model.train_on_batch(noise, positive_y))\n",
    "\n",
    "                # If at save interval => save generated image samples\n",
    "                if epoch % save_interval == 0:\n",
    "                    self.generate_images(output_dir, epoch)\n",
    "                    self.generator.save_weights(os.path.join(output_dir, 'epoch_{}_g.h5'.format(epoch)))\n",
    "                    self.discriminator.save_weights(os.path.join(output_dir, 'epoch_{}_d.h5'.format(epoch)))\n",
    "                    \n",
    "            self.discriminator_losses.append(discriminator_loss)\n",
    "            self.generator_losses.append(generator_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAN Type: gan\n",
      "Optimizer: Adam\n",
      "Loss: Binary Cross Entropy\n",
      "Critics Ratio: 1\n",
      "Epoch:  0\n",
      "Number of batches:  546\n",
      "(128, 28, 28, 1) (128, 100) (128, 1, 1, 1) (128, 1, 1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kojin/anaconda/envs/ml/lib/python3.6/site-packages/keras/engine/training.py:973: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 28, 28, 1) (128, 100) (128, 1, 1, 1) (128, 1, 1, 1)\n",
      "(128, 28, 28, 1) (128, 100) (128, 1, 1, 1) (128, 1, 1, 1)\n",
      "(128, 28, 28, 1) (128, 100) (128, 1, 1, 1) (128, 1, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "dcgan = DCGAN('gan')\n",
    "dcgan.train(20, batch_size=BATCH_SIZE, save_interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
